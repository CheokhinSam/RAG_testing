question,ground_truth,generated_answer,retrieved_context,testset_context
What self-attention do?,"Self-attention, also known as intra-attention, is an attention mechanism that relates different positions of a single sequence to compute a representation of the sequence. It has been successfully used in tasks such as reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.","Self-attention, also known as intra-attention, is an attention mechanism that relates different positions of a single sequence to compute a representation of that sequence. It enables models to capture dependencies between elements at different positions in the sequence. Self-attention has been successfully applied to tasks like reading comprehension, summarization, and textual entailment. It is a key component of the Transformer model, which uses self-attention to compute input and output representations without relying on recurrent or convolutional layers. Self-attention allows for efficient parallelization and reduces the path length for learning long-range dependencies, making it effective for sequence transduction tasks.","[Document(id='683f7d1b-d6ac-45d3-845a-9d61b40474c9', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'headlines': ['Recurrent models typically factor computation along the symbol positions of the input and output', 'sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden', 'states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently', 'sequence lengths, as memory constraints limit batching across examples. Recent work has achieved', 'computation [26], while also improving model performance in case of the latter. The fundamental', 'Attention mechanisms have become an integral part of compelling sequence modeling and transduc-', 'the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms', 'In this work we propose the Transformer, a model architecture eschewing recurrence and instead', 'The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in', 'translation quality after being trained for as little as twelve hours on eight P100 GPUs.', '2 Background', 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU', '[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building', 'block, computing hidden representations in parallel for all input and output positions. In these models,', 'in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes', 'it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is', 'to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as', 'Self-attention, sometimes called intra-attention is an attention mechanism relating different positions', 'of a single sequence in order to compute a representation of the sequence. Self-attention has been', 'End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-', 'To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying', 'aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate', '3 Model Architecture', 'Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].', 'Here, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence', 'of continuous representations z = (z1,...,z n). Given z, the decoder then generates an output', 'sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive', 'The Transformer follows this overall architecture using stacked self-attention and point-wise, fully', 'connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,', '3.1 Encoder and Decoder Stacks', 'Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two', 'sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-']}, page_content='described in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [28].\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate'), Document(id='33dc6adf-5816-4831-8bfa-d07d89c30d8d', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'headlines': ['Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations', 'Layer Type Complexity per Layer Sequential Maximum Path Length', 'Operations', 'Self-Attention O(n2 ·d) O(1) O(1)', 'Recurrent O(n·d2) O(n) O(n)', 'Convolutional O(k·n·d2) O(1) O(logk(n))', 'Self-Attention (restricted) O(r·n·d) O(1) O(n/r)', 'bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel', 'as the embeddings, so that the two can be summed. There are many choices of positional encodings,', 'In this work, we use sine and cosine functions of different frequencies:', 'PE(pos,2i) = sin(pos/100002i/dmodel )', 'PE(pos,2i+1) = cos(pos/100002i/dmodel )', 'where posis the position and iis the dimension. That is, each dimension of the positional encoding', 'corresponds to a sinusoid. The wavelengths form a geometric progression from 2πto 10000 ·2π. We', 'relative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of', 'PEpos.', 'We also experimented with using learned positional embeddings [8] instead, and found that the two', 'versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version', '4 Why Self-Attention', 'In this section we compare various aspects of self-attention layers to the recurrent and convolu-', '(x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,zi ∈Rd, such as a hidden', 'layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we', 'One is the total computational complexity per layer. Another is the amount of computation that can', 'The third is the path length between long-range dependencies in the network. Learning long-range', 'dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the', 'traverse in the network. The shorter these paths between any combination of positions in the input', 'and output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare', 'As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially', 'executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of', '[31] and byte-pair [25] representations. To improve computational performance for tasks involving']}, page_content='during training.\n4 Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,zi ∈Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to'), Document(id='00ebecac-77c7-4a31-8ad9-aeab882369ba', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'headlines': ['Figure 1: The Transformer - model architecture.', 'wise fully connected feed-forward network. We employ a residual connection [10] around each of', 'the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is', 'LayerNorm(x+ Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer', 'itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding', 'Decoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two', 'attention over the output of the encoder stack. Similar to the encoder, we employ residual connections', 'around each of the sub-layers, followed by layer normalization. We also modify the self-attention', 'sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This', '3.2 Attention', 'An attention function can be described as mapping a query and a set of key-value pairs to an output,', 'where the query, keys, values, and output are all vectors. The output is computed as a weighted sum', '3.2.1 Scaled Dot-Product Attention', 'We call our particular attention ""Scaled Dot-Product Attention"" (Figure 2). The input consists of', 'queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the']}, page_content='around each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position ican depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention ""Scaled Dot-Product Attention"" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n3'), Document(id='af4865d9-b995-4038-a2fc-ca9248003bdd', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'headlines': ['Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations', 'Layer Type Complexity per Layer Sequential Maximum Path Length', 'Operations', 'Self-Attention O(n2 ·d) O(1) O(1)', 'Recurrent O(n·d2) O(n) O(n)', 'Convolutional O(k·n·d2) O(1) O(logk(n))', 'Self-Attention (restricted) O(r·n·d) O(1) O(n/r)', 'bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel', 'as the embeddings, so that the two can be summed. There are many choices of positional encodings,', 'In this work, we use sine and cosine functions of different frequencies:', 'PE(pos,2i) = sin(pos/100002i/dmodel )', 'PE(pos,2i+1) = cos(pos/100002i/dmodel )', 'where posis the position and iis the dimension. That is, each dimension of the positional encoding', 'corresponds to a sinusoid. The wavelengths form a geometric progression from 2πto 10000 ·2π. We', 'relative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of', 'PEpos.', 'We also experimented with using learned positional embeddings [8] instead, and found that the two', 'versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version', '4 Why Self-Attention', 'In this section we compare various aspects of self-attention layers to the recurrent and convolu-', '(x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,zi ∈Rd, such as a hidden', 'layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we', 'One is the total computational complexity per layer. Another is the amount of computation that can', 'The third is the path length between long-range dependencies in the network. Learning long-range', 'dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the', 'traverse in the network. The shorter these paths between any combination of positions in the input', 'and output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare', 'As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially', 'executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of', '[31] and byte-pair [25] representations. To improve computational performance for tasks involving']}, page_content='PE(pos,2i) = sin(pos/100002i/dmodel )\nPE(pos,2i+1) = cos(pos/100002i/dmodel )\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto 10000 ·2π. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of\nPEpos.\nWe also experimented with using learned positional embeddings [8] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4 Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-'), Document(id='d4320d55-7528-4f5b-9fc5-54adcc275b8d', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'headlines': ['Recurrent models typically factor computation along the symbol positions of the input and output', 'sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden', 'states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently', 'sequence lengths, as memory constraints limit batching across examples. Recent work has achieved', 'computation [26], while also improving model performance in case of the latter. The fundamental', 'Attention mechanisms have become an integral part of compelling sequence modeling and transduc-', 'the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms', 'In this work we propose the Transformer, a model architecture eschewing recurrence and instead', 'The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in', 'translation quality after being trained for as little as twelve hours on eight P100 GPUs.', '2 Background', 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU', '[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building', 'block, computing hidden representations in parallel for all input and output positions. In these models,', 'in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes', 'it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is', 'to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as', 'Self-attention, sometimes called intra-attention is an attention mechanism relating different positions', 'of a single sequence in order to compute a representation of the sequence. Self-attention has been', 'End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-', 'To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying', 'aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate', '3 Model Architecture', 'Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].', 'Here, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence', 'of continuous representations z = (z1,...,z n). Given z, the decoder then generates an output', 'sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive', 'The Transformer follows this overall architecture using stacked self-attention and point-wise, fully', 'connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,', '3.1 Encoder and Decoder Stacks', 'Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two', 'sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-']}, page_content='The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions')]","['Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved signiﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional computation [26], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28]. To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [14, 15] and [8]. 3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29]. Here, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence of continuous representations z = (z1,...,z n). Given z, the decoder then generates an output sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive [9], consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. 3.1']"
What ByteNet do and how it different from Transformer?,"ByteNet uses convolutional neural networks as its basic building block, computing hidden representations in parallel for all input and output positions. The number of operations required to relate signals from two arbitrary input or output positions grows logarithmically with ByteNet. In contrast, the Transformer relies entirely on self-attention, reducing the number of operations to a constant, albeit with reduced effective resolution due to averaging attention-weighted positions.","ByteNet is a sequence transduction model that uses convolutional neural networks (CNNs as its basic building block) to compute hidden representations in parallel for all input and output positions. It reduces sequential computation, and the number of operations required to relate signals between two arbitrary positions grows logarithmically with their distance. This makes it more efficient than models like ConvS2S, which grow linearly, but it can still struggle to learn dependencies between distant positions.

The Transformer, on the other hand, eschews recurrence and convolution entirely, relying solely on self-attention mechanisms to model global dependencies between input and output. This allows the Transformer to reduce the number of operations required to relate signals between positions to a constant, enabling significantly more parallelization and faster training compared to ByteNet. However, the Transformer may have reduced effective resolution due to averaging attention-weighted positions, which is counteracted using Multi-Head Attention.

In summary, ByteNet uses CNNs with logarithmic growth in operations for distant dependencies, while the Transformer uses self-attention with constant operations, enabling faster training and greater parallelization.","[Document(id='47a3dd9d-61ba-4804-893f-857309fab389', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'headlines': ['Recurrent models typically factor computation along the symbol positions of the input and output', 'sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden', 'states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently', 'sequence lengths, as memory constraints limit batching across examples. Recent work has achieved', 'computation [26], while also improving model performance in case of the latter. The fundamental', 'Attention mechanisms have become an integral part of compelling sequence modeling and transduc-', 'the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms', 'In this work we propose the Transformer, a model architecture eschewing recurrence and instead', 'The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in', 'translation quality after being trained for as little as twelve hours on eight P100 GPUs.', '2 Background', 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU', '[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building', 'block, computing hidden representations in parallel for all input and output positions. In these models,', 'in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes', 'it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is', 'to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as', 'Self-attention, sometimes called intra-attention is an attention mechanism relating different positions', 'of a single sequence in order to compute a representation of the sequence. Self-attention has been', 'End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-', 'To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying', 'aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate', '3 Model Architecture', 'Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].', 'Here, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence', 'of continuous representations z = (z1,...,z n). Given z, the decoder then generates an output', 'sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive', 'The Transformer follows this overall architecture using stacked self-attention and point-wise, fully', 'connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,', '3.1 Encoder and Decoder Stacks', 'Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two', 'sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-']}, page_content='Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building'), Document(id='ec288a11-db94-48e3-aa18-841ff70127a1', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'headlines': ['Recurrent models typically factor computation along the symbol positions of the input and output', 'sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden', 'states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently', 'sequence lengths, as memory constraints limit batching across examples. Recent work has achieved', 'computation [26], while also improving model performance in case of the latter. The fundamental', 'Attention mechanisms have become an integral part of compelling sequence modeling and transduc-', 'the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms', 'In this work we propose the Transformer, a model architecture eschewing recurrence and instead', 'The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in', 'translation quality after being trained for as little as twelve hours on eight P100 GPUs.', '2 Background', 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU', '[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building', 'block, computing hidden representations in parallel for all input and output positions. In these models,', 'in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes', 'it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is', 'to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as', 'Self-attention, sometimes called intra-attention is an attention mechanism relating different positions', 'of a single sequence in order to compute a representation of the sequence. Self-attention has been', 'End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-', 'To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying', 'aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate', '3 Model Architecture', 'Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].', 'Here, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence', 'of continuous representations z = (z1,...,z n). Given z, the decoder then generates an output', 'sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive', 'The Transformer follows this overall architecture using stacked self-attention and point-wise, fully', 'connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,', '3.1 Encoder and Decoder Stacks', 'Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two', 'sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-']}, page_content='aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [14, 15] and [8].\n3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence\nof continuous representations z = (z1,...,z n). Given z, the decoder then generates an output\nsequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive\n[9], consuming the previously generated symbols as additional input when generating the next.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks'), Document(id='d4320d55-7528-4f5b-9fc5-54adcc275b8d', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'headlines': ['Recurrent models typically factor computation along the symbol positions of the input and output', 'sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden', 'states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently', 'sequence lengths, as memory constraints limit batching across examples. Recent work has achieved', 'computation [26], while also improving model performance in case of the latter. The fundamental', 'Attention mechanisms have become an integral part of compelling sequence modeling and transduc-', 'the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms', 'In this work we propose the Transformer, a model architecture eschewing recurrence and instead', 'The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in', 'translation quality after being trained for as little as twelve hours on eight P100 GPUs.', '2 Background', 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU', '[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building', 'block, computing hidden representations in parallel for all input and output positions. In these models,', 'in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes', 'it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is', 'to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as', 'Self-attention, sometimes called intra-attention is an attention mechanism relating different positions', 'of a single sequence in order to compute a representation of the sequence. Self-attention has been', 'End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-', 'To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying', 'aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate', '3 Model Architecture', 'Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].', 'Here, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence', 'of continuous representations z = (z1,...,z n). Given z, the decoder then generates an output', 'sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive', 'The Transformer follows this overall architecture using stacked self-attention and point-wise, fully', 'connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,', '3.1 Encoder and Decoder Stacks', 'Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two', 'sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-']}, page_content='The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions'), Document(id='94126878-436d-4cdc-a9c4-c19b4dfc3c9c', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'headlines': ['Figure 1: The Transformer - model architecture.', 'wise fully connected feed-forward network. We employ a residual connection [10] around each of', 'the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is', 'LayerNorm(x+ Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer', 'itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding', 'Decoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two', 'attention over the output of the encoder stack. Similar to the encoder, we employ residual connections', 'around each of the sub-layers, followed by layer normalization. We also modify the self-attention', 'sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This', '3.2 Attention', 'An attention function can be described as mapping a query and a set of key-value pairs to an output,', 'where the query, keys, values, and output are all vectors. The output is computed as a weighted sum', '3.2.1 Scaled Dot-Product Attention', 'We call our particular attention ""Scaled Dot-Product Attention"" (Figure 2). The input consists of', 'queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the']}, page_content='Figure 1: The Transformer - model architecture.\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\nLayerNorm(x+ Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This'), Document(id='bf480b90-12b1-4ab7-a170-3fda1659c160', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'headlines': ['Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base', 'model. All metrics are on the English-to-German translation development set, newstest2013. Listed', 'N d model dff h d k dv Pdrop ϵls', 'train PPL BLEU params', 'base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65', '(A)', '(B) 16 5.16 25.1 58', '(C)', '(D)', '(E) positional embedding instead of sinusoids 4.92 25.7', 'big 6 1024 4096 16 0.3 300K 4.33 26.4 213', 'In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This', 'function than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,', 'bigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our', '7 Conclusion', 'In this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on', 'For translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based', 'on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014', 'English-to-French translation tasks, we achieve a new state of the art. In the former task our best', 'We are excited about the future of attention-based models and plan to apply them to other tasks. We', 'plan to extend the Transformer to problems involving input and output modalities other than text and', 'such as images, audio and video. Making generation less sequential is another research goals of ours.', 'The code we used to train and evaluate our models is available at https://github.com/', 'Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful']}, page_content='multi-headed self-attention.\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor.\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful')]","['Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved signiﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional computation [26], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28]. To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [14, 15] and [8]. 3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29]. Here, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence of continuous representations z = (z1,...,z n). Given z, the decoder then generates an output sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive [9], consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. 3.1']"
"What do the encoder do in them transformer thing, like how it work with layers and stuff?","The encoder in a Transformer is composed of a stack of N = 6 identical layers. Each layer has two sub-layers: the first is a multi-head self-attention mechanism, and the second is a simple position-wise feed-forward network.","The encoder in a Transformer model is composed of a stack of six identical layers, each with two main sub-layers:

1. **Multi-Head Self-Attention Mechanism**: This allows the model to focus on different parts of the input sequence simultaneously, capturing relationships between tokens regardless of their position in the sequence.

2. **Position-Wise Fully Connected Feed-Forward Network**: This applies two linear transformations with a ReLU activation in between, independently to each position in the sequence.

Each sub-layer is wrapped with a **residual connection** (adding the input of the sub-layer to its output) followed by **layer normalization**. The outputs of all sub-layers and embedding layers have a fixed dimension of 512. Positional encodings are added to the input embeddings to provide information about the order of tokens in the sequence.","[Document(id='42c52f37-2543-48be-b1c3-e67f9d16765e', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'headlines': ['Recurrent models typically factor computation along the symbol positions of the input and output', 'sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden', 'states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently', 'sequence lengths, as memory constraints limit batching across examples. Recent work has achieved', 'computation [26], while also improving model performance in case of the latter. The fundamental', 'Attention mechanisms have become an integral part of compelling sequence modeling and transduc-', 'the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms', 'In this work we propose the Transformer, a model architecture eschewing recurrence and instead', 'The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in', 'translation quality after being trained for as little as twelve hours on eight P100 GPUs.', '2 Background', 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU', '[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building', 'block, computing hidden representations in parallel for all input and output positions. In these models,', 'in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes', 'it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is', 'to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as', 'Self-attention, sometimes called intra-attention is an attention mechanism relating different positions', 'of a single sequence in order to compute a representation of the sequence. Self-attention has been', 'End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-', 'To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying', 'aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate', '3 Model Architecture', 'Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].', 'Here, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence', 'of continuous representations z = (z1,...,z n). Given z, the decoder then generates an output', 'sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive', 'The Transformer follows this overall architecture using stacked self-attention and point-wise, fully', 'connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,', '3.1 Encoder and Decoder Stacks', 'Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two', 'sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-']}, page_content='connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\n2'), Document(id='94126878-436d-4cdc-a9c4-c19b4dfc3c9c', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'headlines': ['Figure 1: The Transformer - model architecture.', 'wise fully connected feed-forward network. We employ a residual connection [10] around each of', 'the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is', 'LayerNorm(x+ Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer', 'itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding', 'Decoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two', 'attention over the output of the encoder stack. Similar to the encoder, we employ residual connections', 'around each of the sub-layers, followed by layer normalization. We also modify the self-attention', 'sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This', '3.2 Attention', 'An attention function can be described as mapping a query and a set of key-value pairs to an output,', 'where the query, keys, values, and output are all vectors. The output is computed as a weighted sum', '3.2.1 Scaled Dot-Product Attention', 'We call our particular attention ""Scaled Dot-Product Attention"" (Figure 2). The input consists of', 'queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the']}, page_content='Figure 1: The Transformer - model architecture.\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\nLayerNorm(x+ Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This'), Document(id='ec288a11-db94-48e3-aa18-841ff70127a1', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'headlines': ['Recurrent models typically factor computation along the symbol positions of the input and output', 'sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden', 'states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently', 'sequence lengths, as memory constraints limit batching across examples. Recent work has achieved', 'computation [26], while also improving model performance in case of the latter. The fundamental', 'Attention mechanisms have become an integral part of compelling sequence modeling and transduc-', 'the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms', 'In this work we propose the Transformer, a model architecture eschewing recurrence and instead', 'The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in', 'translation quality after being trained for as little as twelve hours on eight P100 GPUs.', '2 Background', 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU', '[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building', 'block, computing hidden representations in parallel for all input and output positions. In these models,', 'in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes', 'it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is', 'to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as', 'Self-attention, sometimes called intra-attention is an attention mechanism relating different positions', 'of a single sequence in order to compute a representation of the sequence. Self-attention has been', 'End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-', 'To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying', 'aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate', '3 Model Architecture', 'Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].', 'Here, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence', 'of continuous representations z = (z1,...,z n). Given z, the decoder then generates an output', 'sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive', 'The Transformer follows this overall architecture using stacked self-attention and point-wise, fully', 'connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,', '3.1 Encoder and Decoder Stacks', 'Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two', 'sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-']}, page_content='aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [14, 15] and [8].\n3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence\nof continuous representations z = (z1,...,z n). Given z, the decoder then generates an output\nsequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive\n[9], consuming the previously generated symbols as additional input when generating the next.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks'), Document(id='241a5b33-a26e-4baa-9959-8c3826fb32e3', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'headlines': ['Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations', 'Layer Type Complexity per Layer Sequential Maximum Path Length', 'Operations', 'Self-Attention O(n2 ·d) O(1) O(1)', 'Recurrent O(n·d2) O(n) O(n)', 'Convolutional O(k·n·d2) O(1) O(logk(n))', 'Self-Attention (restricted) O(r·n·d) O(1) O(n/r)', 'bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel', 'as the embeddings, so that the two can be summed. There are many choices of positional encodings,', 'In this work, we use sine and cosine functions of different frequencies:', 'PE(pos,2i) = sin(pos/100002i/dmodel )', 'PE(pos,2i+1) = cos(pos/100002i/dmodel )', 'where posis the position and iis the dimension. That is, each dimension of the positional encoding', 'corresponds to a sinusoid. The wavelengths form a geometric progression from 2πto 10000 ·2π. We', 'relative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of', 'PEpos.', 'We also experimented with using learned positional embeddings [8] instead, and found that the two', 'versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version', '4 Why Self-Attention', 'In this section we compare various aspects of self-attention layers to the recurrent and convolu-', '(x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,zi ∈Rd, such as a hidden', 'layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we', 'One is the total computational complexity per layer. Another is the amount of computation that can', 'The third is the path length between long-range dependencies in the network. Learning long-range', 'dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the', 'traverse in the network. The shorter these paths between any combination of positions in the input', 'and output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare', 'As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially', 'executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of', '[31] and byte-pair [25] representations. To improve computational performance for tasks involving']}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\nLayer Type Complexity per Layer Sequential Maximum Path Length\nOperations\nSelf-Attention O(n2 ·d) O(1) O(1)\nRecurrent O(n·d2) O(n) O(n)\nConvolutional O(k·n·d2) O(1) O(logk(n))\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and ﬁxed [8].\nIn this work, we use sine and cosine functions of different frequencies:\nPE(pos,2i) = sin(pos/100002i/dmodel )\nPE(pos,2i+1) = cos(pos/100002i/dmodel )\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding'), Document(id='ca43e9d5-d87e-4f20-b190-afe9cf3e8e53', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'headlines': ['MultiHead(Q,K,V ) = Concat(head1,..., headh)WO', 'where headi = Attention(QWQ', 'i ,KW K', 'i ,VW V', 'Where the projections are parameter matricesWQ', 'i ∈Rdmodel×dk , WK', 'i ∈Rdmodel×dk , WV', 'i ∈Rdmodel×dv', 'and WO ∈Rhdv×dmodel .', 'In this work we employ h = 8 parallel attention layers, or heads. For each of these we use', 'dk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost', '3.2.3 Applications of Attention in our Model', 'The Transformer uses multi-head attention in three different ways:', '• In ""encoder-decoder attention"" layers, the queries come from the previous decoder layer,', 'and the memory keys and values come from the output of the encoder. This allows every', 'position in the decoder to attend over all positions in the input sequence. This mimics the', '• The encoder contains self-attention layers. In a self-attention layer all of the keys, values', 'encoder. Each position in the encoder can attend to all positions in the previous layer of the', '• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to', 'all positions in the decoder up to and including that position. We need to prevent leftward', 'information ﬂow in the decoder to preserve the auto-regressive property. We implement this', 'of the softmax which correspond to illegal connections. See Figure 2.', '3.3 Position-wise Feed-Forward Networks', 'In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully', 'connected feed-forward network, which is applied to each position separately and identically. This', 'consists of two linear transformations with a ReLU activation in between.', 'FFN(x) = max(0,xW1 + b1)W2 + b2 (2)', 'While the linear transformations are the same across different positions, they use different parameters', 'from layer to layer. Another way of describing this is as two convolutions with kernel size 1.', 'The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality', '3.4 Embeddings and Softmax', 'Similarly to other sequence transduction models, we use learned embeddings to convert the input', 'tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-', 'mation and softmax function to convert the decoder output to predicted next-token probabilities. In', 'linear transformation, similar to [24]. In the embedding layers, we multiply those weights by √dmodel.', '3.5 Positional Encoding', 'Since our model contains no recurrence and no convolution, in order for the model to make use of the', 'tokens in the sequence. To this end, we add ""positional encodings"" to the input embeddings at the']}, page_content='of the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN(x) = max(0,xW1 + b1)W2 + b2 (2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\ndff = 2048.\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-')]","['Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position- 2']"
What is the structure of the encoder in the encoder-decoder stack?,"The encoder is composed of a stack of N = 6 identical layers, each containing two sub-layers: a multi-head self-attention mechanism and a simple position mechanism.","The encoder in the encoder-decoder stack is composed of a stack of **N = 6 identical layers**, where each layer has two sub-layers:

1. **Multi-head self-attention mechanism**: Each position in the encoder can attend to all positions in the previous layer of the encoder.
2. **Position-wise fully connected feed-forward network**.

Each sub-layer is wrapped with a **residual connection**, followed by **layer normalization**. The output of each sub-layer is computed as `LayerNorm(x + Sublayer(x))`. All sub-layers and embedding layers produce outputs of dimension `d_model = 512`.","[Document(id='42c52f37-2543-48be-b1c3-e67f9d16765e', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'headlines': ['Recurrent models typically factor computation along the symbol positions of the input and output', 'sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden', 'states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently', 'sequence lengths, as memory constraints limit batching across examples. Recent work has achieved', 'computation [26], while also improving model performance in case of the latter. The fundamental', 'Attention mechanisms have become an integral part of compelling sequence modeling and transduc-', 'the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms', 'In this work we propose the Transformer, a model architecture eschewing recurrence and instead', 'The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in', 'translation quality after being trained for as little as twelve hours on eight P100 GPUs.', '2 Background', 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU', '[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building', 'block, computing hidden representations in parallel for all input and output positions. In these models,', 'in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes', 'it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is', 'to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as', 'Self-attention, sometimes called intra-attention is an attention mechanism relating different positions', 'of a single sequence in order to compute a representation of the sequence. Self-attention has been', 'End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-', 'To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying', 'aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate', '3 Model Architecture', 'Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].', 'Here, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence', 'of continuous representations z = (z1,...,z n). Given z, the decoder then generates an output', 'sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive', 'The Transformer follows this overall architecture using stacked self-attention and point-wise, fully', 'connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,', '3.1 Encoder and Decoder Stacks', 'Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two', 'sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-']}, page_content='connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\n2'), Document(id='94126878-436d-4cdc-a9c4-c19b4dfc3c9c', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'headlines': ['Figure 1: The Transformer - model architecture.', 'wise fully connected feed-forward network. We employ a residual connection [10] around each of', 'the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is', 'LayerNorm(x+ Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer', 'itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding', 'Decoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two', 'attention over the output of the encoder stack. Similar to the encoder, we employ residual connections', 'around each of the sub-layers, followed by layer normalization. We also modify the self-attention', 'sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This', '3.2 Attention', 'An attention function can be described as mapping a query and a set of key-value pairs to an output,', 'where the query, keys, values, and output are all vectors. The output is computed as a weighted sum', '3.2.1 Scaled Dot-Product Attention', 'We call our particular attention ""Scaled Dot-Product Attention"" (Figure 2). The input consists of', 'queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the']}, page_content='Figure 1: The Transformer - model architecture.\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\nLayerNorm(x+ Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This'), Document(id='ec288a11-db94-48e3-aa18-841ff70127a1', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'headlines': ['Recurrent models typically factor computation along the symbol positions of the input and output', 'sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden', 'states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently', 'sequence lengths, as memory constraints limit batching across examples. Recent work has achieved', 'computation [26], while also improving model performance in case of the latter. The fundamental', 'Attention mechanisms have become an integral part of compelling sequence modeling and transduc-', 'the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms', 'In this work we propose the Transformer, a model architecture eschewing recurrence and instead', 'The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in', 'translation quality after being trained for as little as twelve hours on eight P100 GPUs.', '2 Background', 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU', '[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building', 'block, computing hidden representations in parallel for all input and output positions. In these models,', 'in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes', 'it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is', 'to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as', 'Self-attention, sometimes called intra-attention is an attention mechanism relating different positions', 'of a single sequence in order to compute a representation of the sequence. Self-attention has been', 'End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-', 'To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying', 'aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate', '3 Model Architecture', 'Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].', 'Here, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence', 'of continuous representations z = (z1,...,z n). Given z, the decoder then generates an output', 'sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive', 'The Transformer follows this overall architecture using stacked self-attention and point-wise, fully', 'connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,', '3.1 Encoder and Decoder Stacks', 'Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two', 'sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-']}, page_content='aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [14, 15] and [8].\n3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence\nof continuous representations z = (z1,...,z n). Given z, the decoder then generates an output\nsequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive\n[9], consuming the previously generated symbols as additional input when generating the next.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks'), Document(id='241a5b33-a26e-4baa-9959-8c3826fb32e3', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'headlines': ['Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations', 'Layer Type Complexity per Layer Sequential Maximum Path Length', 'Operations', 'Self-Attention O(n2 ·d) O(1) O(1)', 'Recurrent O(n·d2) O(n) O(n)', 'Convolutional O(k·n·d2) O(1) O(logk(n))', 'Self-Attention (restricted) O(r·n·d) O(1) O(n/r)', 'bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel', 'as the embeddings, so that the two can be summed. There are many choices of positional encodings,', 'In this work, we use sine and cosine functions of different frequencies:', 'PE(pos,2i) = sin(pos/100002i/dmodel )', 'PE(pos,2i+1) = cos(pos/100002i/dmodel )', 'where posis the position and iis the dimension. That is, each dimension of the positional encoding', 'corresponds to a sinusoid. The wavelengths form a geometric progression from 2πto 10000 ·2π. We', 'relative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of', 'PEpos.', 'We also experimented with using learned positional embeddings [8] instead, and found that the two', 'versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version', '4 Why Self-Attention', 'In this section we compare various aspects of self-attention layers to the recurrent and convolu-', '(x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,zi ∈Rd, such as a hidden', 'layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we', 'One is the total computational complexity per layer. Another is the amount of computation that can', 'The third is the path length between long-range dependencies in the network. Learning long-range', 'dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the', 'traverse in the network. The shorter these paths between any combination of positions in the input', 'and output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare', 'As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially', 'executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of', '[31] and byte-pair [25] representations. To improve computational performance for tasks involving']}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\nLayer Type Complexity per Layer Sequential Maximum Path Length\nOperations\nSelf-Attention O(n2 ·d) O(1) O(1)\nRecurrent O(n·d2) O(n) O(n)\nConvolutional O(k·n·d2) O(1) O(logk(n))\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and ﬁxed [8].\nIn this work, we use sine and cosine functions of different frequencies:\nPE(pos,2i) = sin(pos/100002i/dmodel )\nPE(pos,2i+1) = cos(pos/100002i/dmodel )\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding'), Document(id='694211b9-cbcd-43de-acd9-8d0345f0d2d8', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'headlines': ['MultiHead(Q,K,V ) = Concat(head1,..., headh)WO', 'where headi = Attention(QWQ', 'i ,KW K', 'i ,VW V', 'Where the projections are parameter matricesWQ', 'i ∈Rdmodel×dk , WK', 'i ∈Rdmodel×dk , WV', 'i ∈Rdmodel×dv', 'and WO ∈Rhdv×dmodel .', 'In this work we employ h = 8 parallel attention layers, or heads. For each of these we use', 'dk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost', '3.2.3 Applications of Attention in our Model', 'The Transformer uses multi-head attention in three different ways:', '• In ""encoder-decoder attention"" layers, the queries come from the previous decoder layer,', 'and the memory keys and values come from the output of the encoder. This allows every', 'position in the decoder to attend over all positions in the input sequence. This mimics the', '• The encoder contains self-attention layers. In a self-attention layer all of the keys, values', 'encoder. Each position in the encoder can attend to all positions in the previous layer of the', '• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to', 'all positions in the decoder up to and including that position. We need to prevent leftward', 'information ﬂow in the decoder to preserve the auto-regressive property. We implement this', 'of the softmax which correspond to illegal connections. See Figure 2.', '3.3 Position-wise Feed-Forward Networks', 'In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully', 'connected feed-forward network, which is applied to each position separately and identically. This', 'consists of two linear transformations with a ReLU activation in between.', 'FFN(x) = max(0,xW1 + b1)W2 + b2 (2)', 'While the linear transformations are the same across different positions, they use different parameters', 'from layer to layer. Another way of describing this is as two convolutions with kernel size 1.', 'The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality', '3.4 Embeddings and Softmax', 'Similarly to other sequence transduction models, we use learned embeddings to convert the input', 'tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-', 'mation and softmax function to convert the decoder output to predicted next-token probabilities. In', 'linear transformation, similar to [24]. In the embedding layers, we multiply those weights by √dmodel.', '3.5 Positional Encoding', 'Since our model contains no recurrence and no convolution, in order for the model to make use of the', 'tokens in the sequence. To this end, we add ""positional encodings"" to the input embeddings at the']}, page_content='position in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[31, 2, 8].\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks')]","['Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position- 2']"
How does the Transformer encoder differ from recurrent models in handling sequence dependencies?,"The Transformer encoder differs from recurrent models in handling sequence dependencies by relying entirely on a self-attention mechanism instead of sequence-aligned recurrence. This allows the Transformer to model dependencies without regard to their distance in the input or output sequences, enabling significant parallelization. In contrast, recurrent models process sequences sequentially, which precludes parallelization and makes it more challenging to handle long-range dependencies efficiently.","The Transformer encoder differs from recurrent models in handling sequence dependencies by entirely eschewing recurrence and relying on self-attention mechanisms. This allows the Transformer to model dependencies between any positions in the input sequence with a constant number of operations, regardless of their distance. In contrast, recurrent models require sequential operations, which grow linearly with the sequence length, making it harder to learn long-range dependencies. Additionally, the Transformer's reliance on self-attention enables significant parallelization, unlike recurrent models that process sequences step-by-step.","[Document(id='ec288a11-db94-48e3-aa18-841ff70127a1', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'headlines': ['Recurrent models typically factor computation along the symbol positions of the input and output', 'sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden', 'states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently', 'sequence lengths, as memory constraints limit batching across examples. Recent work has achieved', 'computation [26], while also improving model performance in case of the latter. The fundamental', 'Attention mechanisms have become an integral part of compelling sequence modeling and transduc-', 'the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms', 'In this work we propose the Transformer, a model architecture eschewing recurrence and instead', 'The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in', 'translation quality after being trained for as little as twelve hours on eight P100 GPUs.', '2 Background', 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU', '[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building', 'block, computing hidden representations in parallel for all input and output positions. In these models,', 'in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes', 'it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is', 'to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as', 'Self-attention, sometimes called intra-attention is an attention mechanism relating different positions', 'of a single sequence in order to compute a representation of the sequence. Self-attention has been', 'End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-', 'To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying', 'aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate', '3 Model Architecture', 'Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].', 'Here, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence', 'of continuous representations z = (z1,...,z n). Given z, the decoder then generates an output', 'sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive', 'The Transformer follows this overall architecture using stacked self-attention and point-wise, fully', 'connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,', '3.1 Encoder and Decoder Stacks', 'Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two', 'sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-']}, page_content='aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [14, 15] and [8].\n3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence\nof continuous representations z = (z1,...,z n). Given z, the decoder then generates an output\nsequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive\n[9], consuming the previously generated symbols as additional input when generating the next.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks'), Document(id='47a3dd9d-61ba-4804-893f-857309fab389', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'headlines': ['Recurrent models typically factor computation along the symbol positions of the input and output', 'sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden', 'states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently', 'sequence lengths, as memory constraints limit batching across examples. Recent work has achieved', 'computation [26], while also improving model performance in case of the latter. The fundamental', 'Attention mechanisms have become an integral part of compelling sequence modeling and transduc-', 'the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms', 'In this work we propose the Transformer, a model architecture eschewing recurrence and instead', 'The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in', 'translation quality after being trained for as little as twelve hours on eight P100 GPUs.', '2 Background', 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU', '[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building', 'block, computing hidden representations in parallel for all input and output positions. In these models,', 'in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes', 'it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is', 'to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as', 'Self-attention, sometimes called intra-attention is an attention mechanism relating different positions', 'of a single sequence in order to compute a representation of the sequence. Self-attention has been', 'End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-', 'To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying', 'aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate', '3 Model Architecture', 'Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].', 'Here, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence', 'of continuous representations z = (z1,...,z n). Given z, the decoder then generates an output', 'sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive', 'The Transformer follows this overall architecture using stacked self-attention and point-wise, fully', 'connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,', '3.1 Encoder and Decoder Stacks', 'Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two', 'sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-']}, page_content='Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building'), Document(id='d6ff8953-4d76-47f4-9bb8-dcad7be5ebab', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'headlines': ['Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations', 'Layer Type Complexity per Layer Sequential Maximum Path Length', 'Operations', 'Self-Attention O(n2 ·d) O(1) O(1)', 'Recurrent O(n·d2) O(n) O(n)', 'Convolutional O(k·n·d2) O(1) O(logk(n))', 'Self-Attention (restricted) O(r·n·d) O(1) O(n/r)', 'bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel', 'as the embeddings, so that the two can be summed. There are many choices of positional encodings,', 'In this work, we use sine and cosine functions of different frequencies:', 'PE(pos,2i) = sin(pos/100002i/dmodel )', 'PE(pos,2i+1) = cos(pos/100002i/dmodel )', 'where posis the position and iis the dimension. That is, each dimension of the positional encoding', 'corresponds to a sinusoid. The wavelengths form a geometric progression from 2πto 10000 ·2π. We', 'relative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of', 'PEpos.', 'We also experimented with using learned positional embeddings [8] instead, and found that the two', 'versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version', '4 Why Self-Attention', 'In this section we compare various aspects of self-attention layers to the recurrent and convolu-', '(x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,zi ∈Rd, such as a hidden', 'layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we', 'One is the total computational complexity per layer. Another is the amount of computation that can', 'The third is the path length between long-range dependencies in the network. Learning long-range', 'dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the', 'traverse in the network. The shorter these paths between any combination of positions in the input', 'and output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare', 'As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially', 'executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of', '[31] and byte-pair [25] representations. To improve computational performance for tasks involving']}, page_content='dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\nlength n is smaller than the representation dimensionality d, which is most often the case with'), Document(id='d4320d55-7528-4f5b-9fc5-54adcc275b8d', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'headlines': ['Recurrent models typically factor computation along the symbol positions of the input and output', 'sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden', 'states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently', 'sequence lengths, as memory constraints limit batching across examples. Recent work has achieved', 'computation [26], while also improving model performance in case of the latter. The fundamental', 'Attention mechanisms have become an integral part of compelling sequence modeling and transduc-', 'the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms', 'In this work we propose the Transformer, a model architecture eschewing recurrence and instead', 'The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in', 'translation quality after being trained for as little as twelve hours on eight P100 GPUs.', '2 Background', 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU', '[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building', 'block, computing hidden representations in parallel for all input and output positions. In these models,', 'in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes', 'it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is', 'to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as', 'Self-attention, sometimes called intra-attention is an attention mechanism relating different positions', 'of a single sequence in order to compute a representation of the sequence. Self-attention has been', 'End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-', 'To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying', 'aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate', '3 Model Architecture', 'Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].', 'Here, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence', 'of continuous representations z = (z1,...,z n). Given z, the decoder then generates an output', 'sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive', 'The Transformer follows this overall architecture using stacked self-attention and point-wise, fully', 'connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,', '3.1 Encoder and Decoder Stacks', 'Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two', 'sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-']}, page_content='The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions'), Document(id='683f7d1b-d6ac-45d3-845a-9d61b40474c9', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'headlines': ['Recurrent models typically factor computation along the symbol positions of the input and output', 'sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden', 'states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently', 'sequence lengths, as memory constraints limit batching across examples. Recent work has achieved', 'computation [26], while also improving model performance in case of the latter. The fundamental', 'Attention mechanisms have become an integral part of compelling sequence modeling and transduc-', 'the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms', 'In this work we propose the Transformer, a model architecture eschewing recurrence and instead', 'The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in', 'translation quality after being trained for as little as twelve hours on eight P100 GPUs.', '2 Background', 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU', '[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building', 'block, computing hidden representations in parallel for all input and output positions. In these models,', 'in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes', 'it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is', 'to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as', 'Self-attention, sometimes called intra-attention is an attention mechanism relating different positions', 'of a single sequence in order to compute a representation of the sequence. Self-attention has been', 'End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-', 'To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying', 'aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate', '3 Model Architecture', 'Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].', 'Here, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence', 'of continuous representations z = (z1,...,z n). Given z, the decoder then generates an output', 'sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive', 'The Transformer follows this overall architecture using stacked self-attention and point-wise, fully', 'connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,', '3.1 Encoder and Decoder Stacks', 'Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two', 'sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-']}, page_content='described in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [28].\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate')]","['<1-hop>\n\nRecurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved signiﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional computation [26], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28]. To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [14, 15] and [8]. 3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29]. Here, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence of continuous representations z = (z1,...,z n). Given z, the decoder then generates an output sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive [9], consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. 3.1', '<2-hop>\n\nEncoder and Decoder Stacks Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position- 2']"
"How does the Transformer model utilize the multi-head self-attention mechanism to address the limitations of sequential computation in recurrent models, and what advantages does this provide for sequence-based tasks?","The Transformer model addresses the limitations of sequential computation in recurrent models by entirely eschewing recurrence and instead relying on an attention mechanism to draw global dependencies between input and output. Specifically, it employs a multi-head self-attention mechanism, which allows the model to compute representations of the input and output sequences in parallel, significantly improving computational efficiency. Unlike recurrent models, where the number of operations required to relate signals from two arbitrary positions grows with their distance, the Transformer reduces this to a constant number of operations. The multi-head self-attention mechanism further enhances this by enabling the model to focus on different parts of the sequence simultaneously, counteracting the reduced effective resolution caused by averaging attention-weighted positions. This architecture allows the Transformer to model dependencies without regard to their distance in the sequence, enabling faster training and achieving state-of-the-art performance in tasks like translation after as little as twelve hours of training on eight P100 GPUs.","The Transformer model utilizes the multi-head self-attention mechanism to address the limitations of sequential computation in recurrent models by allowing the model to compute global dependencies between input and output sequences without relying on sequence-aligned recurrence. Multi-head self-attention enables the model to process all positions in a sequence in parallel, significantly reducing the number of operations required to relate distant positions to a constant number, unlike recurrent models where such operations grow with sequence length.

This parallelization provides several advantages for sequence-based tasks:
1. **Faster Training**: The Transformer can be trained significantly faster than recurrent models, as it avoids the sequential nature of RNNs and allows for efficient parallel computation.
2. **Better Long-Range Dependency Modeling**: The self-attention mechanism can directly relate all positions in a sequence, making it easier to capture dependencies between distant elements.
3. **State-of-the-Art Performance**: The Transformer achieves superior performance on tasks like machine translation, outperforming previous models based on recurrence or convolution.

In summary, the multi-head self-attention mechanism enables the Transformer to overcome the inefficiencies of sequential computation, leading to faster training and improved performance on sequence-based tasks.","[Document(id='23f3c88e-a30d-4484-9a5f-76097ee728e2', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'headlines': ['MultiHead(Q,K,V ) = Concat(head1,..., headh)WO', 'where headi = Attention(QWQ', 'i ,KW K', 'i ,VW V', 'Where the projections are parameter matricesWQ', 'i ∈Rdmodel×dk , WK', 'i ∈Rdmodel×dk , WV', 'i ∈Rdmodel×dv', 'and WO ∈Rhdv×dmodel .', 'In this work we employ h = 8 parallel attention layers, or heads. For each of these we use', 'dk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost', '3.2.3 Applications of Attention in our Model', 'The Transformer uses multi-head attention in three different ways:', '• In ""encoder-decoder attention"" layers, the queries come from the previous decoder layer,', 'and the memory keys and values come from the output of the encoder. This allows every', 'position in the decoder to attend over all positions in the input sequence. This mimics the', '• The encoder contains self-attention layers. In a self-attention layer all of the keys, values', 'encoder. Each position in the encoder can attend to all positions in the previous layer of the', '• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to', 'all positions in the decoder up to and including that position. We need to prevent leftward', 'information ﬂow in the decoder to preserve the auto-regressive property. We implement this', 'of the softmax which correspond to illegal connections. See Figure 2.', '3.3 Position-wise Feed-Forward Networks', 'In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully', 'connected feed-forward network, which is applied to each position separately and identically. This', 'consists of two linear transformations with a ReLU activation in between.', 'FFN(x) = max(0,xW1 + b1)W2 + b2 (2)', 'While the linear transformations are the same across different positions, they use different parameters', 'from layer to layer. Another way of describing this is as two convolutions with kernel size 1.', 'The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality', '3.4 Embeddings and Softmax', 'Similarly to other sequence transduction models, we use learned embeddings to convert the input', 'tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-', 'mation and softmax function to convert the decoder output to predicted next-token probabilities. In', 'linear transformation, similar to [24]. In the embedding layers, we multiply those weights by √dmodel.', '3.5 Positional Encoding', 'Since our model contains no recurrence and no convolution, in order for the model to make use of the', 'tokens in the sequence. To this end, we add ""positional encodings"" to the input embeddings at the']}, page_content='MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\nwhere headi = Attention(QWQ\ni ,KW K\ni ,VW V\ni )\nWhere the projections are parameter matricesWQ\ni ∈Rdmodel×dk , WK\ni ∈Rdmodel×dk , WV\ni ∈Rdmodel×dv\nand WO ∈Rhdv×dmodel .\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n• In ""encoder-decoder attention"" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[31, 2, 8].'), Document(id='683f7d1b-d6ac-45d3-845a-9d61b40474c9', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'headlines': ['Recurrent models typically factor computation along the symbol positions of the input and output', 'sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden', 'states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently', 'sequence lengths, as memory constraints limit batching across examples. Recent work has achieved', 'computation [26], while also improving model performance in case of the latter. The fundamental', 'Attention mechanisms have become an integral part of compelling sequence modeling and transduc-', 'the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms', 'In this work we propose the Transformer, a model architecture eschewing recurrence and instead', 'The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in', 'translation quality after being trained for as little as twelve hours on eight P100 GPUs.', '2 Background', 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU', '[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building', 'block, computing hidden representations in parallel for all input and output positions. In these models,', 'in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes', 'it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is', 'to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as', 'Self-attention, sometimes called intra-attention is an attention mechanism relating different positions', 'of a single sequence in order to compute a representation of the sequence. Self-attention has been', 'End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-', 'To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying', 'aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate', '3 Model Architecture', 'Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].', 'Here, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence', 'of continuous representations z = (z1,...,z n). Given z, the decoder then generates an output', 'sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive', 'The Transformer follows this overall architecture using stacked self-attention and point-wise, fully', 'connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,', '3.1 Encoder and Decoder Stacks', 'Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two', 'sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-']}, page_content='described in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [28].\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate'), Document(id='d4320d55-7528-4f5b-9fc5-54adcc275b8d', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'headlines': ['Recurrent models typically factor computation along the symbol positions of the input and output', 'sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden', 'states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently', 'sequence lengths, as memory constraints limit batching across examples. Recent work has achieved', 'computation [26], while also improving model performance in case of the latter. The fundamental', 'Attention mechanisms have become an integral part of compelling sequence modeling and transduc-', 'the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms', 'In this work we propose the Transformer, a model architecture eschewing recurrence and instead', 'The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in', 'translation quality after being trained for as little as twelve hours on eight P100 GPUs.', '2 Background', 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU', '[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building', 'block, computing hidden representations in parallel for all input and output positions. In these models,', 'in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes', 'it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is', 'to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as', 'Self-attention, sometimes called intra-attention is an attention mechanism relating different positions', 'of a single sequence in order to compute a representation of the sequence. Self-attention has been', 'End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-', 'To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying', 'aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate', '3 Model Architecture', 'Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].', 'Here, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence', 'of continuous representations z = (z1,...,z n). Given z, the decoder then generates an output', 'sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive', 'The Transformer follows this overall architecture using stacked self-attention and point-wise, fully', 'connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,', '3.1 Encoder and Decoder Stacks', 'Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two', 'sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-']}, page_content='The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions'), Document(id='47a3dd9d-61ba-4804-893f-857309fab389', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'headlines': ['Recurrent models typically factor computation along the symbol positions of the input and output', 'sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden', 'states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently', 'sequence lengths, as memory constraints limit batching across examples. Recent work has achieved', 'computation [26], while also improving model performance in case of the latter. The fundamental', 'Attention mechanisms have become an integral part of compelling sequence modeling and transduc-', 'the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms', 'In this work we propose the Transformer, a model architecture eschewing recurrence and instead', 'The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in', 'translation quality after being trained for as little as twelve hours on eight P100 GPUs.', '2 Background', 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU', '[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building', 'block, computing hidden representations in parallel for all input and output positions. In these models,', 'in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes', 'it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is', 'to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as', 'Self-attention, sometimes called intra-attention is an attention mechanism relating different positions', 'of a single sequence in order to compute a representation of the sequence. Self-attention has been', 'End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-', 'To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying', 'aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate', '3 Model Architecture', 'Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].', 'Here, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence', 'of continuous representations z = (z1,...,z n). Given z, the decoder then generates an output', 'sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive', 'The Transformer follows this overall architecture using stacked self-attention and point-wise, fully', 'connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,', '3.1 Encoder and Decoder Stacks', 'Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two', 'sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-']}, page_content='Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building'), Document(id='bf480b90-12b1-4ab7-a170-3fda1659c160', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'headlines': ['Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base', 'model. All metrics are on the English-to-German translation development set, newstest2013. Listed', 'N d model dff h d k dv Pdrop ϵls', 'train PPL BLEU params', 'base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65', '(A)', '(B) 16 5.16 25.1 58', '(C)', '(D)', '(E) positional embedding instead of sinusoids 4.92 25.7', 'big 6 1024 4096 16 0.3 300K 4.33 26.4 213', 'In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This', 'function than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,', 'bigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our', '7 Conclusion', 'In this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on', 'For translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based', 'on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014', 'English-to-French translation tasks, we achieve a new state of the art. In the former task our best', 'We are excited about the future of attention-based models and plan to apply them to other tasks. We', 'plan to extend the Transformer to problems involving input and output modalities other than text and', 'such as images, audio and video. Making generation less sequential is another research goals of ours.', 'The code we used to train and evaluate our models is available at https://github.com/', 'Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful']}, page_content='multi-headed self-attention.\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor.\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful')]","['<1-hop>\n\nRecurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved signiﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional computation [26], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28]. To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [14, 15] and [8]. 3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29]. Here, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence of continuous representations z = (z1,...,z n). Given z, the decoder then generates an output sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive [9], consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. 3.1', '<2-hop>\n\nEncoder and Decoder Stacks Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position- 2']"
How does the Transformer model utilize the multi-head self-attention mechanism in its encoder to improve sequence representation compared to traditional recurrent models?,"The Transformer model utilizes the multi-head self-attention mechanism in its encoder to improve sequence representation by allowing the model to draw global dependencies between input and output without relying on recurrence. The encoder is composed of a stack of six identical layers, each containing a multi-head self-attention mechanism as the first sub-layer. This mechanism enables the model to compute hidden representations in parallel for all input positions, reducing the number of operations required to relate signals from arbitrary positions to a constant number. This contrasts with traditional recurrent models, which process sequences sequentially, limiting parallelization and making it more difficult to learn dependencies between distant positions. By eschewing recurrence, the Transformer achieves significant improvements in computational efficiency and performance.","The Transformer model utilizes the multi-head self-attention mechanism in its encoder to improve sequence representation by enabling the model to jointly attend to information from different representation subspaces at different positions in the input sequence. Unlike traditional recurrent models, which process sequences sequentially and rely on recurrence to model dependencies, the self-attention mechanism allows the Transformer to compute global dependencies between input elements in parallel. This significantly reduces sequential computation and increases parallelization, making the model more efficient.

Multi-head attention works by projecting the queries, keys, and values multiple times into smaller subspaces (using learned linear projections) and performing attention in parallel across these subspaces. The outputs from these attention heads are then concatenated and projected again to form the final representation. This approach avoids the averaging effect of single-head attention and captures richer, diverse features from the input sequence. Consequently, the Transformer achieves better sequence representations and faster training compared to recurrent models.","[Document(id='23f3c88e-a30d-4484-9a5f-76097ee728e2', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'headlines': ['MultiHead(Q,K,V ) = Concat(head1,..., headh)WO', 'where headi = Attention(QWQ', 'i ,KW K', 'i ,VW V', 'Where the projections are parameter matricesWQ', 'i ∈Rdmodel×dk , WK', 'i ∈Rdmodel×dk , WV', 'i ∈Rdmodel×dv', 'and WO ∈Rhdv×dmodel .', 'In this work we employ h = 8 parallel attention layers, or heads. For each of these we use', 'dk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost', '3.2.3 Applications of Attention in our Model', 'The Transformer uses multi-head attention in three different ways:', '• In ""encoder-decoder attention"" layers, the queries come from the previous decoder layer,', 'and the memory keys and values come from the output of the encoder. This allows every', 'position in the decoder to attend over all positions in the input sequence. This mimics the', '• The encoder contains self-attention layers. In a self-attention layer all of the keys, values', 'encoder. Each position in the encoder can attend to all positions in the previous layer of the', '• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to', 'all positions in the decoder up to and including that position. We need to prevent leftward', 'information ﬂow in the decoder to preserve the auto-regressive property. We implement this', 'of the softmax which correspond to illegal connections. See Figure 2.', '3.3 Position-wise Feed-Forward Networks', 'In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully', 'connected feed-forward network, which is applied to each position separately and identically. This', 'consists of two linear transformations with a ReLU activation in between.', 'FFN(x) = max(0,xW1 + b1)W2 + b2 (2)', 'While the linear transformations are the same across different positions, they use different parameters', 'from layer to layer. Another way of describing this is as two convolutions with kernel size 1.', 'The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality', '3.4 Embeddings and Softmax', 'Similarly to other sequence transduction models, we use learned embeddings to convert the input', 'tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-', 'mation and softmax function to convert the decoder output to predicted next-token probabilities. In', 'linear transformation, similar to [24]. In the embedding layers, we multiply those weights by √dmodel.', '3.5 Positional Encoding', 'Since our model contains no recurrence and no convolution, in order for the model to make use of the', 'tokens in the sequence. To this end, we add ""positional encodings"" to the input embeddings at the']}, page_content='MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\nwhere headi = Attention(QWQ\ni ,KW K\ni ,VW V\ni )\nWhere the projections are parameter matricesWQ\ni ∈Rdmodel×dk , WK\ni ∈Rdmodel×dk , WV\ni ∈Rdmodel×dv\nand WO ∈Rhdv×dmodel .\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n• In ""encoder-decoder attention"" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[31, 2, 8].'), Document(id='ec288a11-db94-48e3-aa18-841ff70127a1', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'headlines': ['Recurrent models typically factor computation along the symbol positions of the input and output', 'sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden', 'states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently', 'sequence lengths, as memory constraints limit batching across examples. Recent work has achieved', 'computation [26], while also improving model performance in case of the latter. The fundamental', 'Attention mechanisms have become an integral part of compelling sequence modeling and transduc-', 'the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms', 'In this work we propose the Transformer, a model architecture eschewing recurrence and instead', 'The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in', 'translation quality after being trained for as little as twelve hours on eight P100 GPUs.', '2 Background', 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU', '[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building', 'block, computing hidden representations in parallel for all input and output positions. In these models,', 'in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes', 'it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is', 'to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as', 'Self-attention, sometimes called intra-attention is an attention mechanism relating different positions', 'of a single sequence in order to compute a representation of the sequence. Self-attention has been', 'End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-', 'To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying', 'aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate', '3 Model Architecture', 'Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].', 'Here, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence', 'of continuous representations z = (z1,...,z n). Given z, the decoder then generates an output', 'sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive', 'The Transformer follows this overall architecture using stacked self-attention and point-wise, fully', 'connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,', '3.1 Encoder and Decoder Stacks', 'Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two', 'sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-']}, page_content='aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [14, 15] and [8].\n3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence\nof continuous representations z = (z1,...,z n). Given z, the decoder then generates an output\nsequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive\n[9], consuming the previously generated symbols as additional input when generating the next.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks'), Document(id='683f7d1b-d6ac-45d3-845a-9d61b40474c9', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'headlines': ['Recurrent models typically factor computation along the symbol positions of the input and output', 'sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden', 'states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently', 'sequence lengths, as memory constraints limit batching across examples. Recent work has achieved', 'computation [26], while also improving model performance in case of the latter. The fundamental', 'Attention mechanisms have become an integral part of compelling sequence modeling and transduc-', 'the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms', 'In this work we propose the Transformer, a model architecture eschewing recurrence and instead', 'The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in', 'translation quality after being trained for as little as twelve hours on eight P100 GPUs.', '2 Background', 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU', '[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building', 'block, computing hidden representations in parallel for all input and output positions. In these models,', 'in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes', 'it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is', 'to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as', 'Self-attention, sometimes called intra-attention is an attention mechanism relating different positions', 'of a single sequence in order to compute a representation of the sequence. Self-attention has been', 'End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-', 'To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying', 'aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate', '3 Model Architecture', 'Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].', 'Here, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence', 'of continuous representations z = (z1,...,z n). Given z, the decoder then generates an output', 'sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive', 'The Transformer follows this overall architecture using stacked self-attention and point-wise, fully', 'connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,', '3.1 Encoder and Decoder Stacks', 'Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two', 'sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-']}, page_content='described in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [28].\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate'), Document(id='da7d4c38-1f1b-4051-b3c9-69cac96a02bb', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'headlines': ['Scaled Dot-Product Attention', ' Multi-Head Attention', 'Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several', 'In practice, we compute the attention function on a set of queries simultaneously, packed together', 'into a matrix Q. The keys and values are also packed together into matrices Kand V. We compute', 'Attention(Q,K,V ) = softmax(QKT', ')V (1)', 'The two most commonly used attention functions are additive attention [2], and dot-product (multi-', 'plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor', '. Additive attention computes the compatibility function using a feed-forward network with', 'a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is', 'While for small values of dk the two mechanisms perform similarly, additive attention outperforms', 'dot product attention without scaling for larger values of dk [3]. We suspect that for large values of', 'extremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk', '3.2.2 Multi-Head Attention', 'Instead of performing a single attention function with dmodel-dimensional keys, values and queries,', 'linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of', 'output values. These are concatenated and once again projected, resulting in the ﬁnal values, as', 'depicted in Figure 2.', 'Multi-head attention allows the model to jointly attend to information from different representation', 'subspaces at different positions. With a single attention head, averaging inhibits this.', '4To illustrate why the dot products get large, assume that the components of q and k are independent random', 'variables with mean 0 and variance 1. Then their dot product, q · k = ∑dk']}, page_content='we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\nvariables with mean 0 and variance 1. Then their dot product, q · k = ∑dk\ni=1 qiki, has mean 0 and variance dk.\n4'), Document(id='47a3dd9d-61ba-4804-893f-857309fab389', metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'Docs/cde.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'headlines': ['Recurrent models typically factor computation along the symbol positions of the input and output', 'sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden', 'states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently', 'sequence lengths, as memory constraints limit batching across examples. Recent work has achieved', 'computation [26], while also improving model performance in case of the latter. The fundamental', 'Attention mechanisms have become an integral part of compelling sequence modeling and transduc-', 'the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms', 'In this work we propose the Transformer, a model architecture eschewing recurrence and instead', 'The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in', 'translation quality after being trained for as little as twelve hours on eight P100 GPUs.', '2 Background', 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU', '[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building', 'block, computing hidden representations in parallel for all input and output positions. In these models,', 'in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes', 'it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is', 'to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as', 'Self-attention, sometimes called intra-attention is an attention mechanism relating different positions', 'of a single sequence in order to compute a representation of the sequence. Self-attention has been', 'End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-', 'To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying', 'aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate', '3 Model Architecture', 'Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].', 'Here, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence', 'of continuous representations z = (z1,...,z n). Given z, the decoder then generates an output', 'sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive', 'The Transformer follows this overall architecture using stacked self-attention and point-wise, fully', 'connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,', '3.1 Encoder and Decoder Stacks', 'Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two', 'sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-']}, page_content='Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building')]","['<1-hop>\n\nRecurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved signiﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional computation [26], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28]. To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [14, 15] and [8]. 3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29]. Here, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence of continuous representations z = (z1,...,z n). Given z, the decoder then generates an output sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive [9], consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. 3.1', '<2-hop>\n\nEncoder and Decoder Stacks Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position- 2']"
